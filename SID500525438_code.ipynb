{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318: Assignment 1\n",
    "## By SID 500525438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as st\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our training data\n",
    "\n",
    "with h5py.File('./Input/train/images_training.h5','r') as H:\n",
    "    data_train = np.copy(H['datatrain'])\n",
    "with h5py.File('./Input/train/labels_training.h5','r') as H:\n",
    "    label_train = np.copy(H['labeltrain'])\n",
    "    \n",
    "# Loading our testing data\n",
    "\n",
    "with h5py.File('./Input/test/images_testing.h5','r') as H:\n",
    "    data_test = np.copy(H['datatest'])\n",
    "with h5py.File('./Input/test/labels_testing_2000.h5','r') as H:\n",
    "    label_test = np.copy(H['labeltest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (30000,)\n",
      "(5000, 784) (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Verifying our loaded training data\n",
    "print(data_train.shape, label_train.shape)\n",
    "\n",
    "# Verifying our loaded testing data\n",
    "print(data_test.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class mappings\n",
    "class_mappings = {\n",
    "    0: 'T-shirt/Top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_top1_accuracy(predicted, actual):\n",
    "    '''\n",
    "    Calculates the top-1 accuracy metric, given the predicted classes against the actual classes\n",
    "    INPUT: 1D array of predicted results,\n",
    "        1D array of actual results\n",
    "    OUTPUT: percentage in decimal format of accuracy    \n",
    "    '''\n",
    "    correct = 0\n",
    "    for n in range(actual.shape[0]):\n",
    "        if predicted[n] == actual[n]:\n",
    "            correct += 1\n",
    "        \n",
    "    return correct/actual.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(data_train, data_test, n_components=45):\n",
    "    '''\n",
    "    Apply PCA on the given dataset\n",
    "    INPUT: 2D or 3D array dataset for data_train and data_test,\n",
    "        INT n_components for the number of components to be retained, or set to STR 'dynamic' to count eigenvalues > 1\n",
    "    OUTPUT: 2D array of dataset reduced to n dimensions\n",
    "    '''\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "    \n",
    "    # Need to get the mean of each feature, for mean normalisation/centreing\n",
    "    data_train_mean = data_train.mean(axis=0)\n",
    "    data_test_mean = data_test.mean(axis=0)\n",
    "    # Feature means should now be zero, or approx. close to zero - and hence centred\n",
    "    data_train_centred = np.subtract(data_train, data_train_mean)\n",
    "    data_test_centred = np.subtract(data_test, data_test_mean)\n",
    "    \n",
    "    # Checking the following, we can see that the max and min value of the entire matrix is 0 and 1\n",
    "    # hence scaling is not required\n",
    "    '''\n",
    "    print(data_train.min())\n",
    "    print(data_train.max())\n",
    "    print(data_test.min())\n",
    "    print(data_test.max())\n",
    "    '''\n",
    "    \n",
    "    covariance_matrix = (data_train_centred.T).dot(data_train_centred)\n",
    "    l, V = np.linalg.eig(covariance_matrix)\n",
    "    \n",
    "    sorted_lambda_index =  l.argsort()[::-1] # sorting our lambda values from largest to smallest\n",
    "    \n",
    "    if n_components == 'dynamic':\n",
    "        # The number of components to be retained for PCA, will be based on the number of eigenvalues which are > 1\n",
    "        n_components = np.count_nonzero(l > 1)\n",
    "        print(f'Number of factors retained is: {n_components}')\n",
    "        \n",
    "    V_n = V[:,sorted_lambda_index[:n_components]]\n",
    "    \n",
    "    # Do the projection of the image matrix against our orthogonal eigenvector matrix reduced to n columns\n",
    "    pca_data_train = data_train_centred.dot(V_n)\n",
    "    pca_data_test = data_test_centred.dot(V_n)\n",
    "    \n",
    "    return (pca_data_train, pca_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svd(data_train, data_test, n_components=False):\n",
    "    '''\n",
    "    Apply Singular Value Decomposition for a given training dataset,\n",
    "    and subsequently apply our test dataset onto the same orthognal V\n",
    "    '''\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "        \n",
    "    U, s, Vt = np.linalg.svd(data_train, full_matrices=False)\n",
    "        \n",
    "    if n_components == 'dynamic':\n",
    "        # to dynamically set what our n_components value will be\n",
    "        # based on the number of singular values required to reach an energy level, where 1D array s is squared,\n",
    "        # such that we get at least 90% of the total sum squared of s        \n",
    "        s_squared = np.square(s)\n",
    "        s_squared_sum = s_squared.sum()\n",
    "        running_squared_sum = 0\n",
    "        n_components = 0\n",
    "        for idx, sing_val_sqrd in enumerate(s_squared):\n",
    "            running_squared_sum += sing_val_sqrd\n",
    "            if (running_squared_sum/s_squared_sum) > 0.9:\n",
    "                n_components = idx + 1\n",
    "                break\n",
    "        print(f'Number of components retained is: {n_components}')\n",
    "        \n",
    "    # Since our singular values in array s are already sorted from largest to smallest, we can then remove\n",
    "    # the insignificant singular values, and also remove the affected rows & columns from U and Vt\n",
    "    # However, since we'll only use Vt, then we only apply it there\n",
    "    if isinstance(n_components, int):\n",
    "        Vt = Vt[:n_components, :]\n",
    "    \n",
    "    # We do dot product between our data matrix and Vt.T because Vt is already reduced to the orthonormal vectors\n",
    "    # which have the highest singular value scores.  Moreover, we need to transpose Vt in order to do dot product\n",
    "    # with our data matrix (data_train and/or data_test)\n",
    "    svd_data_train = data_train.dot(Vt.T)\n",
    "    svd_data_test = data_test.dot(Vt.T)\n",
    "    \n",
    "    return (svd_data_train, svd_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(data_train, label_train, data_test, K=3):\n",
    "    '''\n",
    "    k-Nearest Neighbour classifier\n",
    "    INPUT: 2D/3D array of training dataset (data_train),\n",
    "        1D array of label of training dataset (label_train),\n",
    "        2D/3D array of the dataset to be predicted (data_test),\n",
    "        (optional) K number of nearest neighbours\n",
    "    OUTPUT: 1D array of predicted results with the same length as data_test.shape[0]\n",
    "    '''\n",
    "    \n",
    "    # Reshaping our input data, to ensure it's 2D\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "        \n",
    "    # Instantiating our empty array for predicted values\n",
    "    pred_test = np.zeros(data_test.shape[0])\n",
    "    \n",
    "    for image_num in range(data_test.shape[0]):\n",
    "        # Calculating the distance difference between the test subject and all our training points\n",
    "        sum_sqrd_distances = np.sqrt((np.square(np.subtract(data_train, data_test[image_num]))).sum(axis=1))\n",
    "        #sum_sqrd_distances = np.linalg.norm(data_train - data_test[image_num], axis=1)\n",
    "    \n",
    "        # Getting the k nearest neighbours\n",
    "        k_nearest_neighbours = (np.argsort(sum_sqrd_distances))[:K]\n",
    "    \n",
    "        classes_dict = {}\n",
    "\n",
    "        # Using weighted distance, instead of simply using count\n",
    "        for neighbour_idx in k_nearest_neighbours:\n",
    "            classification = label_train[neighbour_idx]\n",
    "            if classification in classes_dict:\n",
    "                classes_dict[classification] += 1/(sum_sqrd_distances[neighbour_idx]**2)\n",
    "            else:\n",
    "                classes_dict[classification] = 1/(sum_sqrd_distances[neighbour_idx]**2)\n",
    "            \n",
    "        pred_class = None\n",
    "        for key in classes_dict:\n",
    "            if pred_class == None:\n",
    "                pred_class = key\n",
    "                continue\n",
    "\n",
    "            if classes_dict[key] > classes_dict[pred_class]:\n",
    "                pred_class = key\n",
    "                \n",
    "        pred_test[image_num] = pred_class\n",
    "            \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2020-10-14 17:34:57.436159\n",
      "Finished at: 2020-10-14 17:34:57.437164\n",
      "Accuracy result for kNN (raw) is: 0.8275\n",
      "Started at: 2020-10-14 17:34:57.439160\n",
      "Number of factors retained is: 773\n",
      "Finished at: 2020-10-14 17:34:58.906156\n",
      "Accuracy result for kNN (PCA) is: 0.8275\n",
      "Started at: 2020-10-14 17:34:58.907158\n",
      "Number of components retained is: 16\n",
      "Finished at: 2020-10-14 17:35:36.331720\n",
      "Accuracy result for kNN (SVD) is: 0.8205\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbours Classifier using raw data as input\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "knn_results = knn(data_train, label_train, data_test, K=5)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(knn_results, label_test)\n",
    "print(f\"Accuracy result for kNN (raw) is: {accuracy}\")\n",
    "print()\n",
    "\n",
    "# k-Nearest Neighbours Classifier with PCA\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "pca_data_train, pca_data_test = apply_pca(data_train, data_test, n_components='dynamic')\n",
    "knn_pca_results = knn(pca_data_train, label_train, pca_data_test, K=5)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(knn_pca_results, label_test)\n",
    "print(f\"Accuracy result for kNN (PCA) is: {accuracy}\")\n",
    "print()\n",
    "\n",
    "# k-Nearest Neighbours Classifier with SVD\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "svd_data_train, svd_data_test = apply_svd(data_train, data_test, n_components='dynamic')\n",
    "knn_svd_results = knn(svd_data_train, label_train, svd_data_test, K=5)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(knn_svd_results, label_test)\n",
    "print(f\"Accuracy result for kNN (SVD) is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_naive_bayes(data_train, label_train, data_test):\n",
    "    '''\n",
    "    Gaussian Naive Bayes classifier\n",
    "    INPUT: 2D/3D array of training dataset (data_train),\n",
    "        1D array of label on training dataset (label_train),\n",
    "        2D/3D array of test dataset (data_test)\n",
    "    OUTPUT: 1D array of predicted classes on test dataset\n",
    "    '''\n",
    "    \n",
    "    # Reshaping if it's not the expected shape (2D)\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "\n",
    "    # Obtaining the different classes that we have present in our training data and getting index positions of each one\n",
    "    class_indices = {}\n",
    "    for idx, image_class in enumerate(label_train):\n",
    "        if image_class not in class_indices:\n",
    "            class_indices[image_class] = [idx]\n",
    "            continue\n",
    "        else:\n",
    "            class_indices[image_class].append(idx)\n",
    "        \n",
    "    class_mean = {}\n",
    "    class_var = {}\n",
    "\n",
    "    # Obtain the mean and std dev for each class of our training data\n",
    "    for class_index in class_indices:\n",
    "        class_mean[class_index] = data_train[class_indices[class_index], :].mean(axis=0)\n",
    "        class_var[class_index] = data_train[class_indices[class_index], :].var(axis=0)\n",
    "\n",
    "    pred_test = np.zeros(data_test.shape[0])\n",
    "\n",
    "    for image_num in range(data_test.shape[0]):\n",
    "        # In order to find the length of pred_class_scores, we need to get the max value of the keys\n",
    "        # with the assumption that each number up to the max will be a class\n",
    "        # we do this instead of length because our training data may not have an entry for a class, hence, it'll\n",
    "        # result in out of range if a data exists for one higher\n",
    "        pred_class_scores = np.zeros(max(class_indices, key=int)+1)\n",
    "        \n",
    "        for class_index in class_indices:\n",
    "            \n",
    "            # Calculating the logged prior probability\n",
    "            class_prob = np.log(len(class_indices[class_index])/data_train.shape[0])\n",
    "\n",
    "            # Calculating the sum of the logged conditional probability\n",
    "            likelihood_array = st.norm.logpdf(x=data_test[image_num], loc=class_mean[class_index], scale=np.sqrt(class_var[class_index]))\n",
    "            class_prob = class_prob + np.nansum(likelihood_array) # we use nansum to avoid nan likelihoods, because these are obtained from points with zero variance\n",
    "\n",
    "            # Storing the result in our results array, so we can keep track of which class has the highest\n",
    "            pred_class_scores[class_index] = class_prob\n",
    "\n",
    "        # Class with the highest prob is the predicted class for the image, which is stored in our final pred_test array\n",
    "        pred_test[image_num] = np.nanargmax(pred_class_scores)\n",
    "        \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2020-10-14 17:37:17.476474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1782: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:899: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  return (a <= x) & (x <= b)\n",
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:899: RuntimeWarning: invalid value encountered in less_equal\n",
      "  return (a <= x) & (x <= b)\n",
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1782: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at: 2020-10-14 17:37:30.763480\n",
      "Accuracy result for NB (raw) is: 0.655\n",
      "\n",
      "Started at: 2020-10-14 17:37:30.766473\n",
      "Number of factors retained is: 773\n",
      "Finished at: 2020-10-14 17:37:45.919475\n",
      "Accuracy result for NB (PCA) is: 0.4755\n",
      "\n",
      "Started at: 2020-10-14 17:37:45.922472\n",
      "Number of components retained is: 16\n",
      "Finished at: 2020-10-14 17:38:00.401474\n",
      "Accuracy result for kNN (SVD) is: 0.7385\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes using raw data as input\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "nb_results = gaussian_naive_bayes(data_train, label_train, data_test)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(nb_results, label_test)\n",
    "print(f\"Accuracy result for NB (raw) is: {accuracy}\")\n",
    "print()\n",
    "\n",
    "# Gaussian Naive Bayes applied on principal components of dataset\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "pca_data_train, pca_data_test = apply_pca(data_train, data_test, n_components='dynamic')\n",
    "nb_pca_results = gaussian_naive_bayes(pca_data_train, label_train, pca_data_test)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(nb_pca_results, label_test)        \n",
    "print(f\"Accuracy result for NB (PCA) is: {accuracy}\")\n",
    "print()\n",
    "\n",
    "# Gaussian Naive Bayes applied on SVD of dataset\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "svd_data_train, svd_data_test = apply_svd(data_train, data_test, n_components='dynamic')\n",
    "nb_svd_results = gaussian_naive_bayes(svd_data_train, label_train, svd_data_test)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(nb_svd_results, label_test)\n",
    "print(f\"Accuracy result for kNN (SVD) is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
