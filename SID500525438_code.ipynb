{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318: Assignment 1\n",
    "## By SID 500525438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as st\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our training data\n",
    "\n",
    "with h5py.File('./Input/train/images_training.h5','r') as H:\n",
    "    data_train = np.copy(H['datatrain'])\n",
    "with h5py.File('./Input/train/labels_training.h5','r') as H:\n",
    "    label_train = np.copy(H['labeltrain'])\n",
    "    \n",
    "# Loading our testing data\n",
    "\n",
    "with h5py.File('./Input/test/images_testing.h5','r') as H:\n",
    "    data_test = np.copy(H['datatest'])\n",
    "with h5py.File('./Input/test/labels_testing_2000.h5','r') as H:\n",
    "    label_test = np.copy(H['labeltest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (30000,)\n",
      "(5000, 784) (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Verifying our loaded training data\n",
    "print(data_train.shape, label_train.shape)\n",
    "\n",
    "# Verifying our loaded testing data\n",
    "print(data_test.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class mappings\n",
    "class_mappings = {\n",
    "    0: 'T-shirt/Top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_image = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing loading an image\n",
    "\n",
    "data_train = data_train.reshape((data_train.shape[0], 28, 28))\n",
    "plt.imshow(data_train[current_image], cmap=plt.get_cmap('gray'))\n",
    "plt.title(\"class \" + str(label_train[current_image]) + \": \" + class_mappings[label_train[current_image]] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_top1_accuracy(predicted, actual):\n",
    "    '''\n",
    "    Calculates the top-1 accuracy metric, given the predicted classes against the actual classes\n",
    "    INPUT: 1D array of predicted results,\n",
    "        1D array of actual results\n",
    "    OUTPUT: percentage in decimal format of accuracy    \n",
    "    '''\n",
    "    correct = 0\n",
    "    for n in range(actual.shape[0]):\n",
    "        if predicted[n] == actual[n]:\n",
    "            correct += 1\n",
    "        \n",
    "    return correct/actual.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(data_train, data_test, n_components=20):\n",
    "    '''\n",
    "    Apply PCA on the given dataset\n",
    "    INPUT: 2D or 3D array dataset\n",
    "    OUTPUT: 2D array of dataset reduced to n dimensions\n",
    "    '''\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "    \n",
    "    # Need to get the mean of each feature, for mean normalisation/centreing\n",
    "    data_train_mean = data_train.mean(axis=0)\n",
    "    data_test_mean = data_test.mean(axis=0)\n",
    "    # Feature means should now be zero, or approx. close to zero - and hence centred\n",
    "    data_train_centred = np.subtract(data_train, data_train_mean)\n",
    "    data_test_centred = np.subtract(data_test, data_test_mean)\n",
    "    \n",
    "    # Checking the following, we can see that the max and min value of the entire matrix is 0 and 1\n",
    "    # hence scaling is not required\n",
    "    '''\n",
    "    print(data_train.min())\n",
    "    print(data_train.max())\n",
    "    print(data_test.min())\n",
    "    print(data_test.max())\n",
    "    '''\n",
    "    \n",
    "    covariance_matrix = (data_train_centred.T).dot(data_train_centred)\n",
    "    l, V = np.linalg.eig(covariance_matrix)\n",
    "    \n",
    "    sorted_lambda_index =  l.argsort()[::-1] # sorting our lambda values from largest to smallest\n",
    "    \n",
    "    V_n = V[:,sorted_lambda_index[:n_components]]\n",
    "    \n",
    "    # Do the projection of the image matrix against our orthogonal eigenvector matrix reduced to n columns\n",
    "    pca_data_train = data_train_centred.dot(V_n)\n",
    "    pca_data_test = data_test_centred.dot(V_n)\n",
    "    \n",
    "    return (pca_data_train, pca_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svd(data_train, data_test, n_components=False):\n",
    "    '''\n",
    "    Apply Singular Value Decomposition for a given training dataset,\n",
    "    and subsequently apply our test dataset onto the same orthognal V\n",
    "    '''\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "        \n",
    "    U, s, Vt = np.linalg.svd(data_train, full_matrices=False)\n",
    "    \n",
    "    # Since our singular values in array s are already sorted from largest to smallest, we can then remove\n",
    "    # the insignificant singular values, and also remove the affected rows & columns from U and Vt\n",
    "    # However, since we'll only use Vt, then we only apply it there\n",
    "    if n_components:\n",
    "        Vt = Vt[:n_components, :]\n",
    "    \n",
    "    # We do dot product between our data matrix and Vt.T because Vt is already reduced to the orthonormal vectors\n",
    "    # which have the highest singular value scores.  Moreover, we need to transpose Vt in order to do dot product\n",
    "    # with our data matrix (data_train and/or data_test)\n",
    "    svd_data_train = data_train.dot(Vt.T)\n",
    "    svd_data_test = data_test.dot(Vt.T)\n",
    "    \n",
    "    return (svd_data_train, svd_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(data_train, label_train, data_test, K=3):\n",
    "    '''\n",
    "    k-Nearest Neighbour classifier\n",
    "    INPUT: 2D/3D array of training dataset (data_train),\n",
    "        1D array of label of training dataset (label_train),\n",
    "        2D/3D array of the dataset to be predicted (data_test),\n",
    "        (optional) K number of nearest neighbours\n",
    "    OUTPUT: 1D array of predicted results with the same length as data_test.shape[0]\n",
    "    '''\n",
    "    \n",
    "    # Reshaping our input data, to ensure it's 2D\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "        \n",
    "    # Instantiating our empty array for predicted values\n",
    "    pred_test = np.zeros(data_test.shape[0])\n",
    "    \n",
    "    for image_num in range(data_test.shape[0]):\n",
    "        # Calculating the distance difference between the test subject and all our training points\n",
    "        sum_sqrd_distances = np.sqrt((np.square(np.subtract(data_train, data_test[image_num]))).sum(axis=1))\n",
    "        #sum_sqrd_distances = np.linalg.norm(data_train - data_test[image_num], axis=1)\n",
    "    \n",
    "        # Getting the k nearest neighbours\n",
    "        k_nearest_neighbours = (np.argsort(sum_sqrd_distances))[:K]\n",
    "    \n",
    "        classes_dict = {}\n",
    "\n",
    "        # Using weighted distance, instead of simply using count\n",
    "        for neighbour_idx in k_nearest_neighbours:\n",
    "            classification = label_train[neighbour_idx]\n",
    "            if classification in classes_dict:\n",
    "                classes_dict[classification] += 1/(sum_sqrd_distances[neighbour_idx]**2)\n",
    "            else:\n",
    "                classes_dict[classification] = 1/(sum_sqrd_distances[neighbour_idx]**2)\n",
    "            \n",
    "        pred_class = None\n",
    "        for key in classes_dict:\n",
    "            if pred_class == None:\n",
    "                pred_class = key\n",
    "                continue\n",
    "\n",
    "            if classes_dict[key] > classes_dict[pred_class]:\n",
    "                pred_class = key\n",
    "                \n",
    "        pred_test[image_num] = pred_class\n",
    "            \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2020-10-14 13:51:57.903713\n",
      "Finished at: 2020-10-14 14:01:43.121852\n",
      "Accuracy result for kNN (raw) is: 0.8275\n",
      "Started at: 2020-10-14 14:01:43.121852\n",
      "Finished at: 2020-10-14 14:02:30.757156\n",
      "Accuracy result for kNN (PCA) is: 0.8365\n",
      "Started at: 2020-10-14 14:02:30.757156\n",
      "Finished at: 2020-10-14 14:03:22.030093\n",
      "Accuracy result for kNN (SVD) is: 0.8375\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbours Classifier using raw data as input\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "knn_results = knn(data_train, label_train, data_test, K=5)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(knn_results, label_test)\n",
    "print(f\"Accuracy result for kNN (raw) is: {accuracy}\")\n",
    "\n",
    "# k-Nearest Neighbours Classifier with PCA\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "pca_data_train, pca_data_test = apply_pca(data_train, data_test, n_components=50)\n",
    "knn_pca_results = knn(pca_data_train, label_train, pca_data_test, K=5)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(knn_pca_results, label_test)\n",
    "print(f\"Accuracy result for kNN (PCA) is: {accuracy}\")\n",
    "\n",
    "# k-Nearest Neighbours Classifier with SVD\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "svd_data_train, svd_data_test = apply_svd(data_train, data_test, n_components=50)\n",
    "knn_svd_results = knn(svd_data_train, label_train, svd_data_test, K=5)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(knn_svd_results, label_test)\n",
    "print(f\"Accuracy result for kNN (SVD) is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_naive_bayes(data_train, label_train, data_test):\n",
    "    '''\n",
    "    Gaussian Naive Bayes classifier\n",
    "    INPUT: 2D/3D array of training dataset (data_train),\n",
    "        1D array of label on training dataset (label_train),\n",
    "        2D/3D array of test dataset (data_test)\n",
    "    OUTPUT: 1D array of predicted classes on test dataset\n",
    "    '''\n",
    "    \n",
    "    # Reshaping if it's not the expected shape (2D)\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "\n",
    "    # Obtaining the different classes that we have present in our training data and getting index positions of each one\n",
    "    class_indices = {}\n",
    "    for idx, image_class in enumerate(label_train):\n",
    "        if image_class not in class_indices:\n",
    "            class_indices[image_class] = [idx]\n",
    "            continue\n",
    "        else:\n",
    "            class_indices[image_class].append(idx)\n",
    "        \n",
    "    class_mean = {}\n",
    "    class_var = {}\n",
    "\n",
    "    # Obtain the mean and std dev for each class of our training data\n",
    "    for class_index in class_indices:\n",
    "        class_mean[class_index] = data_train[class_indices[class_index], :].mean(axis=0)\n",
    "        class_var[class_index] = data_train[class_indices[class_index], :].var(axis=0)\n",
    "\n",
    "    pred_test = np.zeros(data_test.shape[0])\n",
    "\n",
    "    for image_num in range(data_test.shape[0]):\n",
    "        # In order to find the length of pred_class_scores, we need to get the max value of the keys\n",
    "        # with the assumption that each number up to the max will be a class\n",
    "        # we do this instead of length because our training data may not have an entry for a class, hence, it'll\n",
    "        # result in out of range if a data exists for one higher\n",
    "        pred_class_scores = np.zeros(max(class_indices, key=int)+1)\n",
    "        \n",
    "        for class_index in class_indices:\n",
    "            \n",
    "            # Calculating the logged prior probability\n",
    "            class_prob = np.log(len(class_indices[class_index])/data_train.shape[0])\n",
    "\n",
    "            # Calculating the sum of the logged conditional probability\n",
    "            likelihood_array = st.norm.logpdf(x=data_test[image_num], loc=class_mean[class_index], scale=np.sqrt(class_var[class_index]))\n",
    "            class_prob = class_prob + np.nansum(likelihood_array) # we use nansum to avoid nan likelihoods, because these are obtained from points with zero variance\n",
    "\n",
    "            # Storing the result in our results array, so we can keep track of which class has the highest\n",
    "            pred_class_scores[class_index] = class_prob\n",
    "\n",
    "        # Class with the highest prob is the predicted class for the image, which is stored in our final pred_test array\n",
    "        pred_test[image_num] = np.nanargmax(pred_class_scores)\n",
    "        \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2020-10-14 14:03:22.117095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1782: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:899: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  return (a <= x) & (x <= b)\n",
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:899: RuntimeWarning: invalid value encountered in less_equal\n",
      "  return (a <= x) & (x <= b)\n",
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1782: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at: 2020-10-14 14:03:36.962230\n",
      "Accuracy result for NB (raw) is: 0.655\n",
      "Started at: 2020-10-14 14:03:36.979353\n",
      "Finished at: 2020-10-14 14:03:48.552767\n",
      "Accuracy result for NB (PCA) is: 0.757\n",
      "Started at: 2020-10-14 14:03:48.552767\n",
      "Finished at: 2020-10-14 14:04:03.970988\n",
      "Accuracy result for kNN (SVD) is: 0.7575\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes using raw data as input\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "nb_results = gaussian_naive_bayes(data_train, label_train, data_test)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(nb_results, label_test)\n",
    "print(f\"Accuracy result for NB (raw) is: {accuracy}\")\n",
    "\n",
    "# Gaussian Naive Bayes applied on principal components of dataset\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "pca_data_train, pca_data_test = apply_pca(data_train, data_test, n_components=80)\n",
    "nb_pca_results = gaussian_naive_bayes(pca_data_train, label_train, pca_data_test)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(nb_pca_results, label_test)        \n",
    "print(f\"Accuracy result for NB (PCA) is: {accuracy}\")\n",
    "\n",
    "# Gaussian Naive Bayes applied on SVD of dataset\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "svd_data_train, svd_data_test = apply_svd(data_train, data_test, n_components=80)\n",
    "nb_svd_results = gaussian_naive_bayes(svd_data_train, label_train, svd_data_test)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "accuracy = calc_top1_accuracy(nb_svd_results, label_test)\n",
    "print(f\"Accuracy result for kNN (SVD) is: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
