{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318: Assignment 1\n",
    "## By SID 500525438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our training data\n",
    "\n",
    "with h5py.File('./Input/images_training.h5','r') as H:\n",
    "    data_train = np.copy(H['datatrain'])\n",
    "with h5py.File('./Input/labels_training.h5','r') as H:\n",
    "    label_train = np.copy(H['labeltrain'])\n",
    "    \n",
    "# Loading our testing data\n",
    "\n",
    "with h5py.File('./Input/images_testing.h5','r') as H:\n",
    "    data_test = np.copy(H['datatest'])\n",
    "with h5py.File('./Input/labels_testing_2000.h5','r') as H:\n",
    "    label_test = np.copy(H['labeltest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (30000,)\n",
      "(5000, 784) (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Verifying our loaded training data\n",
    "print(data_train.shape, label_train.shape)\n",
    "\n",
    "# Verifying our loaded testing data\n",
    "print(data_test.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class mappings\n",
    "class_mappings = {\n",
    "    0: 'T-shirt/Top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_top1_accuracy(predicted, actual):\n",
    "    '''\n",
    "    Calculates the top-1 accuracy metric, given the predicted classes against the actual classes\n",
    "    INPUT: 1D array of predicted results,\n",
    "        1D array of actual results\n",
    "    OUTPUT: percentage in decimal format of accuracy    \n",
    "    '''\n",
    "    correct = 0\n",
    "    for n in range(actual.shape[0]):\n",
    "        if predicted[n] == actual[n]:\n",
    "            correct += 1\n",
    "        \n",
    "    return correct/actual.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_prediction(predictions):\n",
    "    '''\n",
    "    Generates the output file for the predictions\n",
    "    INPUT: 1D array of predicted results\n",
    "    OUTPUT: prediction file of .h5 format\n",
    "    '''\n",
    "    with h5py.File('./Output/predicted_labels.h5','w') as H:\n",
    "        H.create_dataset('Output',data=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validation_split(train_ratio, data_train, label_train):\n",
    "    '''\n",
    "    Splits the dataset for X and y into training and validation datasets based on the given ratio\n",
    "    INPUT: train split ratio, training dataset, label dataset\n",
    "    OUTPUT: training data and label numpy arrays, validation data and label numpy arrays\n",
    "    '''\n",
    "    # obtain the row number where we conduct the split\n",
    "    row_split = ratio*data_train.shape[0]\n",
    "    \n",
    "    # shuffle our matrices\n",
    "    np.random.shuffle(data_train)\n",
    "    np.random.shuffle(label_train)\n",
    "    \n",
    "    # create the train/test split\n",
    "    split_data_train = data_train[:row_split]\n",
    "    split_label_train = label_train[:row_split]\n",
    "    split_data_validation = data_train[row_split:]\n",
    "    split_label_validation = label_train[row_split:]\n",
    "    \n",
    "    return split_data_train, split_label_train, split_data_validation, split_label_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(data_train, data_test, n_components=45):\n",
    "    '''\n",
    "    Apply PCA on the given dataset\n",
    "    INPUT: 2D or 3D array dataset for data_train and data_test,\n",
    "        INT n_components for the number of components to be retained, or set to STR 'dynamic' to count eigenvalues > 1\n",
    "    OUTPUT: 2D array of dataset reduced to n dimensions\n",
    "    '''\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "    \n",
    "    # Need to get the mean of each feature, for mean normalisation/centreing\n",
    "    data_train_mean = data_train.mean(axis=0)\n",
    "    data_test_mean = data_test.mean(axis=0)\n",
    "    # Feature means should now be zero, or approx. close to zero - and hence centred\n",
    "    data_train_centred = np.subtract(data_train, data_train_mean)\n",
    "    data_test_centred = np.subtract(data_test, data_test_mean)\n",
    "    \n",
    "    # Checking the following, we can see that the max and min value of the entire matrix is 0 and 1\n",
    "    # hence scaling is not required\n",
    "    '''\n",
    "    print(data_train.min())\n",
    "    print(data_train.max())\n",
    "    print(data_test.min())\n",
    "    print(data_test.max())\n",
    "    '''\n",
    "    \n",
    "    covariance_matrix = (data_train_centred.T).dot(data_train_centred)\n",
    "    l, V = np.linalg.eig(covariance_matrix)\n",
    "    \n",
    "    sorted_lambda_index =  l.argsort()[::-1] # sorting our lambda values from largest to smallest\n",
    "    \n",
    "    if n_components == 'dynamic':\n",
    "        # The number of components to be retained for PCA, will be based on the number of eigenvalues which are > 1\n",
    "        n_components = np.count_nonzero(l > 1)\n",
    "        print(f'Number of factors retained is: {n_components}')\n",
    "        \n",
    "    V_n = V[:,sorted_lambda_index[:n_components]]\n",
    "    \n",
    "    # Do the projection of the image matrix against our orthogonal eigenvector matrix reduced to n columns\n",
    "    pca_data_train = data_train_centred.dot(V_n)\n",
    "    pca_data_test = data_test_centred.dot(V_n)\n",
    "    \n",
    "    return (pca_data_train, pca_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svd(data_train, data_test, n_components=45):\n",
    "    '''\n",
    "    Apply Singular Value Decomposition for a given training dataset,\n",
    "    and subsequently apply our test dataset onto the same orthognal V\n",
    "    '''\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "        \n",
    "    U, s, Vt = np.linalg.svd(data_train, full_matrices=False)\n",
    "        \n",
    "    if n_components == 'dynamic':\n",
    "        # to dynamically set what our n_components value will be\n",
    "        # based on the number of singular values required to reach an energy level, where 1D array s is squared,\n",
    "        # such that we get at least 90% of the total sum squared of s        \n",
    "        s_squared = np.square(s)\n",
    "        s_squared_sum = s_squared.sum()\n",
    "        running_squared_sum = 0\n",
    "        n_components = 0\n",
    "        for idx, sing_val_sqrd in enumerate(s_squared):\n",
    "            running_squared_sum += sing_val_sqrd\n",
    "            if (running_squared_sum/s_squared_sum) > 0.9:\n",
    "                n_components = idx + 1\n",
    "                break\n",
    "        print(f'Number of components retained is: {n_components}')\n",
    "        \n",
    "    # Since our singular values in array s are already sorted from largest to smallest, we can then remove\n",
    "    # the insignificant singular values, and also remove the affected rows & columns from U and Vt\n",
    "    # However, since we'll only use Vt, then we only apply it there\n",
    "    if isinstance(n_components, int):\n",
    "        Vt = Vt[:n_components, :]\n",
    "    \n",
    "    # We do dot product between our data matrix and Vt.T because Vt is already reduced to the orthonormal vectors\n",
    "    # which have the highest singular value scores.  Moreover, we need to transpose Vt in order to do dot product\n",
    "    # with our data matrix (data_train and/or data_test)\n",
    "    svd_data_train = data_train.dot(Vt.T)\n",
    "    svd_data_test = data_test.dot(Vt.T)\n",
    "    \n",
    "    return (svd_data_train, svd_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(data_train, label_train, data_test, K=3):\n",
    "    '''\n",
    "    k-Nearest Neighbour classifier\n",
    "    INPUT: 2D/3D array of training dataset (data_train),\n",
    "        1D array of label of training dataset (label_train),\n",
    "        2D/3D array of the dataset to be predicted (data_test),\n",
    "        (optional) K number of nearest neighbours\n",
    "    OUTPUT: 1D array of predicted results with the same length as data_test.shape[0]\n",
    "    '''\n",
    "    \n",
    "    # Reshaping our input data, to ensure it's 2D\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "        \n",
    "    # Instantiating our empty array for predicted values\n",
    "    pred_test = np.zeros(data_test.shape[0])\n",
    "    \n",
    "    for image_num in range(data_test.shape[0]):\n",
    "        # Calculating the distance difference between the test subject and all our training points\n",
    "        sum_sqrd_distances = np.sqrt((np.square(np.subtract(data_train, data_test[image_num]))).sum(axis=1))\n",
    "        #sum_sqrd_distances = np.linalg.norm(data_train - data_test[image_num], axis=1)\n",
    "    \n",
    "        # Getting the k nearest neighbours\n",
    "        k_nearest_neighbours = (np.argsort(sum_sqrd_distances))[:K]\n",
    "    \n",
    "        classes_dict = {}\n",
    "\n",
    "        # Using weighted distance, instead of simply using count\n",
    "        for neighbour_idx in k_nearest_neighbours:\n",
    "            classification = label_train[neighbour_idx]\n",
    "            if classification in classes_dict:\n",
    "                classes_dict[classification] += 1/(sum_sqrd_distances[neighbour_idx]**2)\n",
    "            else:\n",
    "                classes_dict[classification] = 1/(sum_sqrd_distances[neighbour_idx]**2)\n",
    "            \n",
    "        pred_class = None\n",
    "        for key in classes_dict:\n",
    "            if pred_class == None:\n",
    "                pred_class = key\n",
    "                continue\n",
    "\n",
    "            if classes_dict[key] > classes_dict[pred_class]:\n",
    "                pred_class = key\n",
    "                \n",
    "        pred_test[image_num] = pred_class\n",
    "            \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-Nearest Neighbours Classifier using raw data as input\n",
    "start = time.time()\n",
    "knn_results = knn(data_train, label_train, data_test, K=5)\n",
    "end = time.time()\n",
    "print(f\"Time taken is {end-start} seconds\")\n",
    "accuracy = calc_top1_accuracy(knn_results, label_test)\n",
    "print(f\"Accuracy result for kNN (raw) is: {accuracy}\")\n",
    "print()\n",
    "\n",
    "# k-Nearest Neighbours Classifier with PCA\n",
    "start = time.time()\n",
    "pca_data_train, pca_data_test = apply_pca(data_train, data_test, n_components=43)\n",
    "knn_pca_results = knn(pca_data_train, label_train, pca_data_test, K=5)\n",
    "end = time.time()\n",
    "print(f\"Time taken is {end-start} seconds\")\n",
    "accuracy = calc_top1_accuracy(knn_pca_results, label_test)\n",
    "print(f\"Accuracy result for kNN (PCA) is: {accuracy}\")\n",
    "print()\n",
    "\n",
    "# k-Nearest Neighbours Classifier with SVD\n",
    "start = time.time()\n",
    "svd_data_train, svd_data_test = apply_svd(data_train, data_test, n_components=43)\n",
    "knn_svd_results = knn(svd_data_train, label_train, svd_data_test, K=5)\n",
    "end = time.time()\n",
    "print(f\"Time taken is {end-start} seconds\")\n",
    "accuracy = calc_top1_accuracy(knn_svd_results, label_test)\n",
    "print(f\"Accuracy result for kNN (SVD) is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_naive_bayes(data_train, label_train, data_test):\n",
    "    '''\n",
    "    Gaussian Naive Bayes classifier\n",
    "    INPUT: 2D/3D array of training dataset (data_train),\n",
    "        1D array of label on training dataset (label_train),\n",
    "        2D/3D array of test dataset (data_test)\n",
    "    OUTPUT: 1D array of predicted classes on test dataset\n",
    "    '''\n",
    "    \n",
    "    # Reshaping if it's not the expected shape (2D)\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "\n",
    "    # Obtaining the different classes that we have present in our training data and getting index positions of each one\n",
    "    class_indices = {}\n",
    "    for idx, image_class in enumerate(label_train):\n",
    "        if image_class not in class_indices:\n",
    "            class_indices[image_class] = [idx]\n",
    "            continue\n",
    "        else:\n",
    "            class_indices[image_class].append(idx)\n",
    "        \n",
    "    class_mean = {}\n",
    "    class_var = {}\n",
    "\n",
    "    # Obtain the mean and std dev for each class of our training data\n",
    "    for class_index in class_indices:\n",
    "        class_mean[class_index] = data_train[class_indices[class_index], :].mean(axis=0)\n",
    "        class_var[class_index] = data_train[class_indices[class_index], :].var(axis=0)\n",
    "\n",
    "    pred_test = np.zeros(data_test.shape[0])\n",
    "\n",
    "    for image_num in range(data_test.shape[0]):\n",
    "        # In order to find the length of pred_class_scores, we need to get the max value of the keys\n",
    "        # with the assumption that each number up to the max will be a class\n",
    "        # we do this instead of length because our training data may not have an entry for a class, hence, it'll\n",
    "        # result in out of range if a data exists for one higher\n",
    "        pred_class_scores = np.zeros(max(class_indices, key=int)+1)\n",
    "        \n",
    "        for class_index in class_indices:\n",
    "            \n",
    "            # Calculating the logged prior probability\n",
    "            class_prob = np.log(len(class_indices[class_index])/data_train.shape[0])\n",
    "\n",
    "            # Calculating the sum of the logged conditional probability\n",
    "            likelihood_array = st.norm.logpdf(x=data_test[image_num], loc=class_mean[class_index], scale=np.sqrt(class_var[class_index]))\n",
    "            class_prob = class_prob + np.nansum(likelihood_array) # we use nansum to avoid nan likelihoods, because these are obtained from points with zero variance\n",
    "\n",
    "            # Storing the result in our results array, so we can keep track of which class has the highest\n",
    "            pred_class_scores[class_index] = class_prob\n",
    "\n",
    "        # Class with the highest prob is the predicted class for the image, which is stored in our final pred_test array\n",
    "        pred_test[image_num] = np.nanargmax(pred_class_scores)\n",
    "        \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2020-10-18 21:57:45.457845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1782: RuntimeWarning: invalid value encountered in true_divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n",
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:899: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  return (a <= x) & (x <= b)\n",
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:899: RuntimeWarning: invalid value encountered in less_equal\n",
      "  return (a <= x) & (x <= b)\n",
      "c:\\users\\jsnar\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1782: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  x = np.asarray((x - loc)/scale, dtype=dtyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at: 2020-10-18 21:57:54.406042\n",
      "Accuracy result for NB (raw) is: 0.655\n",
      "\n",
      "Started at: 2020-10-18 21:57:54.406042\n",
      "Number of factors retained is: 773\n",
      "Finished at: 2020-10-18 21:58:06.194436\n",
      "Accuracy result for NB (PCA) is: 0.4755\n",
      "\n",
      "Started at: 2020-10-18 21:58:06.210030\n",
      "Finished at: 2020-10-18 21:58:18.971830\n",
      "Accuracy result for kNN (SVD) is: 0.7665\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes using raw data as input\n",
    "start = time.time()\n",
    "nb_results = gaussian_naive_bayes(data_train, label_train, data_test)\n",
    "end = time.time()\n",
    "print(f\"Time taken is {end-start} seconds\")\n",
    "accuracy = calc_top1_accuracy(nb_results, label_test)\n",
    "print(f\"Accuracy result for NB (raw) is: {accuracy}\")\n",
    "print()\n",
    "\n",
    "# Gaussian Naive Bayes applied on principal components of dataset\n",
    "start = time.time()\n",
    "pca_data_train, pca_data_test = apply_pca(data_train, data_test, n_components='dynamic')\n",
    "nb_pca_results = gaussian_naive_bayes(pca_data_train, label_train, pca_data_test)\n",
    "end = time.time()\n",
    "print(f\"Time taken is {end-start} seconds\")\n",
    "accuracy = calc_top1_accuracy(nb_pca_results, label_test)        \n",
    "print(f\"Accuracy result for NB (PCA) is: {accuracy}\")\n",
    "print()\n",
    "\n",
    "# Gaussian Naive Bayes applied on SVD of dataset\n",
    "start = time.time()\n",
    "svd_data_train, svd_data_test = apply_svd(data_train, data_test, n_components=50)\n",
    "nb_svd_results = gaussian_naive_bayes(svd_data_train, label_train, svd_data_test)\n",
    "end = time.time()\n",
    "print(f\"Time taken is {end-start} seconds\")\n",
    "accuracy = calc_top1_accuracy(nb_svd_results, label_test)\n",
    "print(f\"Accuracy result for NB (SVD) is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "output_prediction(nb_svd_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_logistic_loss(weights, X, y):\n",
    "    \n",
    "    # produce vector z, which is what we'll feed into the sigmoid function\n",
    "    z = np.dot(X, weights)\n",
    "    exp_z = np.exp(z)\n",
    "    \n",
    "    # obtaining a vector on difference between our probability against actual value of y\n",
    "    # which we then apply a broadcast multiplication against matrix X, to create N by D matrix\n",
    "    # and then we sum it to obtain the sum of loss for each feature\n",
    "    dloss = np.sum((exp_z/(1+exp_z) - y) * X, axis=0)\n",
    "    \n",
    "    # calculating the loss\n",
    "    loss = -np.sum(-np.log(1+exp_z) + (y*z))\n",
    "    \n",
    "    return dloss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "\n",
    "X0 = data_train[np.where(label_train==0)[0], :]\n",
    "X1 = data_train[np.where(label_train==1)[0], :]\n",
    "y0 = np.zeros(X0.shape[0])\n",
    "y1 = np.ones(X1.shape[0])\n",
    "\n",
    "X = np.concatenate((X0, X1), axis=0)\n",
    "y = np.concatenate((y0, y1))[:, np.newaxis]\n",
    "\n",
    "N, D = X.shape\n",
    "\n",
    "# select a random starting point for our stochastic gradient descent\n",
    "weights = np.random.random(D)[:, np.newaxis]\n",
    "print(weights.shape)\n",
    "\n",
    "learning_rate = 0.1\n",
    "max_iter = 1000\n",
    "\n",
    "loss_array = []\n",
    "\n",
    "for epoch in range(max_iter):\n",
    "    dloss, current_loss = calc_logistic_loss(weights, X, y)\n",
    "    weights = weights - (learning_rate * dloss/N)\n",
    "    \n",
    "    loss_array.append(current_loss)\n",
    "\n",
    "print(loss_array)\n",
    "    \n",
    "plt.plot(list(range(1,max_iter+1)), loss_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
