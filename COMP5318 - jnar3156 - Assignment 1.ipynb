{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318: Assignment 1\n",
    "## By Jesse S. Narvasa (jnar3156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as st\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our training data\n",
    "\n",
    "with h5py.File('./Input/train/images_training.h5','r') as H:\n",
    "    data_train = np.copy(H['datatrain'])\n",
    "with h5py.File('./Input/train/labels_training.h5','r') as H:\n",
    "    label_train = np.copy(H['labeltrain'])\n",
    "    \n",
    "# Loading our testing data\n",
    "\n",
    "with h5py.File('./Input/test/images_testing.h5','r') as H:\n",
    "    data_test = np.copy(H['datatest'])\n",
    "with h5py.File('./Input/test/labels_testing_2000.h5','r') as H:\n",
    "    label_test = np.copy(H['labeltest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (30000,)\n",
      "(5000, 784) (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Verifying our loaded training data\n",
    "print(data_train.shape, label_train.shape)\n",
    "\n",
    "# Verifying our loaded testing data\n",
    "print(data_test.shape, label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class mappings\n",
    "class_mappings = {\n",
    "    0: 'T-shirt/Top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_image = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loading an image\n",
    "\n",
    "data_train = data_train.reshape((data_train.shape[0], 28, 28))\n",
    "plt.imshow(data_train[current_image], cmap=plt.get_cmap('gray'))\n",
    "plt.title(\"class \" + str(label_train[current_image]) + \": \" + class_mappings[label_train[current_image]] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SVD on it\n",
    "target_data = data_train[current_image]\n",
    "U, s, Vt = np.linalg.svd(target_data, full_matrices=False)\n",
    "S = np.diag(s)\n",
    "\n",
    "# We then create a dynamic component_num in which we'll use to determine how many components we'll keep\n",
    "# this is determine by round to 2 decimal places, and any value that is 0 will be removed\n",
    "svd_components = np.count_nonzero(np.round(s, decimals=2) > 0)\n",
    "\n",
    "target_data_reconstructed = U[:, :svd_components]\\\n",
    "    .dot(S[:svd_components, :svd_components])\\\n",
    "    .dot(Vt[:svd_components, :])\n",
    "\n",
    "plt.imshow(target_data_reconstructed, cmap=plt.get_cmap('gray'))\n",
    "plt.title(f\"After SVD using only the top {svd_components} elements\")\n",
    "plt.show()\n",
    "\n",
    "# Verifying that our reconstructed matrix is approximately equal to our original matrix (within tolerance)\n",
    "print(f'Is the original and reconstructed matrix approximately equal? {np.allclose(target_data, target_data_reconstructed)}')\n",
    "\n",
    "# Checking the compression ratio of this\n",
    "comp_ratio = ((U.shape[0] * svd_components) + (svd_components) + (svd_components * Vt.shape[1])) / (target_data.shape[0]*target_data.shape[1])\n",
    "print(f\"Our compression ratio is: {comp_ratio}\")\n",
    "\n",
    "# SSE\n",
    "svd_SSE = np.sum((target_data - target_data_reconstructed)**2)\n",
    "print(f\"SVD SSE is: {svd_SSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying mean-centering to the original dataset\n",
    "target_data_mean = target_data.mean(axis=0)\n",
    "target_data_mean_matrix = np.full((len(target_data_mean), len(target_data_mean)), target_data_mean)\n",
    "target_data_centred = target_data - target_data_mean_matrix\n",
    "\n",
    "# Now let's try PCA\n",
    "XtX = (target_data_centred.T).dot(target_data_centred)\n",
    "l, V = np.linalg.eig(XtX)\n",
    "L = np.diag(l)\n",
    "\n",
    "pca_components = np.count_nonzero(np.round(l, decimals=2) > 0)\n",
    "\n",
    "pca_projection = target_data_centred.dot(V)\n",
    "target_data_reconstructed = pca_projection[:, :pca_components]\\\n",
    "    .dot(V.T[:pca_components, :])\\\n",
    "    + target_data_mean_matrix\n",
    "\n",
    "plt.imshow(target_data_reconstructed, cmap=plt.get_cmap('gray'))\n",
    "plt.title(f\"After PCA, using only {pca_components} components\")\n",
    "plt.show()\n",
    "\n",
    "# SSE\n",
    "pca_SSE = np.sum((target_data - target_data_reconstructed)**2)\n",
    "print(f\"PCA SSE is: {pca_SSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(data_train, data_test, n_components=20):\n",
    "    '''\n",
    "    Apply PCA on the given dataset\n",
    "    INPUT: 2D or 3D array dataset\n",
    "    OUTPUT: 2D array of dataset reduced to n dimensions\n",
    "    '''\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    \n",
    "    # Need to get the mean of each feature, for mean normalisation/centreing\n",
    "    data_train_mean = data_train.mean(axis=0)\n",
    "    data_test_mean = data_test.mean(axis=0)\n",
    "    # Feature means should now be zero, or approx. close to zero - and hence centred\n",
    "    data_train_centred = np.subtract(data_train, data_train_mean)\n",
    "    data_test_centred = np.subtract(data_test, data_test_mean)\n",
    "    \n",
    "    # Checking the following, we can see that the max and min value of the entire matrix is 0 and 1\n",
    "    # hence scaling is not required\n",
    "    print(data_train.min())\n",
    "    print(data_train.max())\n",
    "    print(data_test.min())\n",
    "    print(data_test.max())\n",
    "    \n",
    "    covariance_matrix = (data_train_centred.T).dot(data_train_centred)\n",
    "    l, V = np.linalg.eig(covariance_matrix)\n",
    "    \n",
    "    sorted_lambda_index =  l.argsort()[::-1] # sorting our lambda values from largest to smallest\n",
    "    \n",
    "    V_n = V[:,sorted_lambda_index[:n_components]]\n",
    "    \n",
    "    # Do the projection of the image matrix against our orthogonal eigenvector matrix reduced to n columns\n",
    "    pca_data_train = data_train_centred.dot(V_n)\n",
    "    pca_data_test = data_test_centred.dot(V_n)\n",
    "    \n",
    "    return (pca_data_train, pca_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(data_train, label_train, data_test, K=3):\n",
    "    '''\n",
    "    k-Nearest Neighbour classifier\n",
    "    INPUT: 2D/3D array of training dataset (data_train),\n",
    "        1D array of label of training dataset (label_train),\n",
    "        2D/3D array of the dataset to be predicted (data_test),\n",
    "        (optional) K number of nearest neighbours\n",
    "    OUTPUT: 1D array of predicted results with the same length as data_test.shape[0]\n",
    "    '''\n",
    "    \n",
    "    # Reshaping our input data, to ensure it's 2D\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "        \n",
    "    # Instantiating our empty array for predicted values\n",
    "    pred_test = np.zeros(data_test.shape[0])\n",
    "    \n",
    "    for image_num in range(data_test.shape[0]):\n",
    "        # Calculating the distance difference between the test subject and all our training points\n",
    "        sum_sqrd_distances = np.sqrt((np.square(np.subtract(data_train, data_test[image_num]))).sum(axis=1))\n",
    "        #sum_sqrd_distances = np.linalg.norm(data_train - data_test[image_num], axis=1)\n",
    "    \n",
    "        # Getting the k nearest neighbours\n",
    "        k_nearest_neighbours = (np.argsort(sum_sqrd_distances))[:K]\n",
    "    \n",
    "        classes_dict = {}\n",
    "\n",
    "        # Using weighted distance, instead of simply using count\n",
    "        for neighbour_idx in k_nearest_neighbours:\n",
    "            classification = label_train[neighbour_idx]\n",
    "            if classification in classes_dict:\n",
    "                classes_dict[classification] += 1/(sum_sqrd_distances[neighbour_idx]**2)\n",
    "            else:\n",
    "                classes_dict[classification] = 1/(sum_sqrd_distances[neighbour_idx]**2)\n",
    "            \n",
    "        pred_class = None\n",
    "        for key in classes_dict:\n",
    "            if pred_class == None:\n",
    "                pred_class = key\n",
    "                continue\n",
    "\n",
    "            if classes_dict[key] > classes_dict[pred_class]:\n",
    "                pred_class = key\n",
    "                \n",
    "        pred_test[image_num] = pred_class\n",
    "            \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2020-10-14 09:19:39.572564\n",
      "Finished at: 2020-10-14 09:28:46.866907\n",
      "Started at: 2020-10-14 09:28:46.866907\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "Finished at: 2020-10-14 09:30:02.559619\n"
     ]
    }
   ],
   "source": [
    "# k-Nearest Neighbours Classifier using raw data as input\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "knn_results = knn(data_train, label_train, data_test, K=5)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "\n",
    "# k-Nearest Neighbours Classifier with PCA\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "pca_data_train, pca_data_test = apply_pca(data_train, data_test, n_components=100)\n",
    "knn_pca_results = knn(pca_data_train, label_train, pca_data_test, K=5)\n",
    "print(f\"Finished at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy result for kNN (raw) is: 0.8275\n",
      "Accuracy result for kNN (PCA) is: 0.8375\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for n in range(label_test.shape[0]):\n",
    "    if knn_results[n] == label_test[n]:\n",
    "        correct += 1\n",
    "        \n",
    "print(f\"Accuracy result for kNN (raw) is: {correct/label_test.shape[0]}\")\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for n in range(label_test.shape[0]):\n",
    "    if knn_pca_results[n] == label_test[n]:\n",
    "        correct += 1\n",
    "        \n",
    "print(f\"Accuracy result for kNN (PCA) is: {correct/label_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_naive_bayes(data_train, label_train, data_test):\n",
    "    '''\n",
    "    Gaussian Naive Bayes classifier\n",
    "    INPUT: 2D/3D array of training dataset (data_train),\n",
    "        1D array of label on training dataset (label_train),\n",
    "        2D/3D array of test dataset (data_test)\n",
    "    OUTPUT: 1D array of predicted classes on test dataset\n",
    "    '''\n",
    "    \n",
    "    # Reshaping if it's not the expected shape (2D)\n",
    "    if len(data_train.shape) != 2:\n",
    "        data_train = data_train.reshape((data_train.shape[0], data_train.shape[1]**2))\n",
    "    if len(data_test.shape) != 2:\n",
    "        data_test = data_test.reshape((data_test.shape[0], data_test.shape[1]**2))\n",
    "\n",
    "    # Obtaining the different classes that we have present in our training data and getting index positions of each one\n",
    "    class_indices = {}\n",
    "    for idx, image_class in enumerate(label_train):\n",
    "        if image_class not in class_indices:\n",
    "            class_indices[image_class] = [idx]\n",
    "            continue\n",
    "        else:\n",
    "            class_indices[image_class].append(idx)\n",
    "        \n",
    "    class_mean = {}\n",
    "    class_var = {}\n",
    "\n",
    "    # Obtain the mean and std dev for each class of our training data\n",
    "    for class_index in class_indices:\n",
    "        class_mean[class_index] = data_train[class_indices[class_index], :].mean(axis=0)\n",
    "        class_var[class_index] = data_train[class_indices[class_index], :].var(axis=0)\n",
    "\n",
    "    pred_test = np.zeros(data_test.shape[0])\n",
    "\n",
    "    for image_num in range(data_test.shape[0]):\n",
    "        # In order to find the length of pred_class_scores, we need to get the max value of the keys\n",
    "        # with the assumption that each number up to the max will be a class\n",
    "        # we do this instead of length because our training data may not have an entry for a class, hence, it'll\n",
    "        # result in out of range if a data exists for one higher\n",
    "        pred_class_scores = np.zeros(max(class_indices, key=int)+1)\n",
    "        \n",
    "        for class_index in class_indices:\n",
    "            \n",
    "            # Calculating the logged prior probability\n",
    "            class_prob = np.log(len(class_indices[class_index])/data_train.shape[0])\n",
    "\n",
    "            # Calculating the sum of the logged conditional probability\n",
    "            likelihood_array = st.norm.logpdf(x=data_test[image_num], loc=class_mean[class_index], scale=np.sqrt(class_var[class_index]))\n",
    "            class_prob = class_prob + np.nansum(likelihood_array) # we use nansum to avoid nan likelihoods, because these are obtained from points with zero variance\n",
    "\n",
    "            # Storing the result in our results array, so we can keep track of which class has the highest\n",
    "            pred_class_scores[class_index] = class_prob\n",
    "\n",
    "        # Class with the highest prob is the predicted class for the image, which is stored in our final pred_test array\n",
    "        pred_test[image_num] = np.nanargmax(pred_class_scores)\n",
    "        \n",
    "    return pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2020-10-14 09:17:52.016334\n",
      "Finished at: 2020-10-14 09:18:02.679172\n",
      "Started at: 2020-10-14 09:18:02.679172\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "Finished at: 2020-10-14 09:18:11.127397\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes using raw data as input\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "nb_results = gaussian_naive_bayes(data_train, label_train, data_test)\n",
    "print(f\"Finished at: {datetime.now()}\")\n",
    "\n",
    "# Gaussian Naive Bayes applied on principal components of dataset\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "pca_data_train, pca_data_test = apply_pca(data_train, data_test, n_components=80)\n",
    "nb_pca_results = gaussian_naive_bayes(pca_data_train, label_train, pca_data_test)\n",
    "print(f\"Finished at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy result for NB (raw) is: 0.655\n",
      "Accuracy result for NB (PCA) is: 0.757\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for n in range(label_test.shape[0]):\n",
    "    if nb_results[n] == label_test[n]:\n",
    "        correct += 1\n",
    "        \n",
    "print(f\"Accuracy result for NB (raw) is: {correct/label_test.shape[0]}\")\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for n in range(label_test.shape[0]):\n",
    "    if nb_pca_results[n] == label_test[n]:\n",
    "        correct += 1\n",
    "        \n",
    "print(f\"Accuracy result for NB (PCA) is: {correct/label_test.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
